---
project: gnn
device: gpu:1 #Can be 'cpu', 'gpu', 'tpu', 'ipu' (with an added ':#number of devices', default, 1 [ex: 'gpu:2]') or auto
problem: mcp
test_enabled: Yes

data:
    path_dataset: datasets # Path where datasets are stored, default data/
    train: # Train/Val data generation parameters
        num_examples_train: 10000
        num_examples_val: 1000
        n_vertices: 100
        sparsify: None #Only works for not fgnns. Put to None if you don't want sparsifying, to an int if you do
        problems:
            tsp:
                distance_used: EUC_2D
                generative_model: Square01
            mcp:
                edge_density: 0.5
                clique_size: 15 #To not plant a clique, use 0
            
    test: #Test data generation parameters
        num_examples_test: 1000
        n_vertices: 200
        sparsify: None
        problems:
            tsp:
                distance_used: EUC_2D
                generative_model: Square01
            mcp:
                edge_density: 0.5
                clique_size: 20 #To not plant a clique, use 0

train: # Training parameters, you can add any Pytorch Lightning Trainer Arguments and pytorch DataLoader arguments
    max_epochs: 100
    batch_size:  64
    optim_args:
        lr: !!float 1e-4
        scheduler_step: 1
        scheduler_factor: 0.5
        lr_stop: !!float 1e-7
        monitor: val_loss #Value to monitor for the scheduler (For now, only val_loss)
    anew: Yes
    start_model: observers/gnn_tsp/1rxn08tk/checkpoints/epoch=4-step=499.ckpt

arch: # Architecture and model
    use_dgl: No
    name: fgnn #fgnn, gatedgcn
    embedding: edge #node or edge
    configs:
        fgnn:
            num_blocks: 3
            original_features_num: 2 
            in_features: 64
            out_features: 1
            depth_of_mlp: 3
            input_embed: Yes
        gatedgcn:
            n_layers: 4
            in_dim: 1
            in_dim_edge: 1
            hidden_dim: 20
            n_classes: 2

observers:
    use: Yes
    base_dir: observers #Name of the path where loggers will save their files if needed
    observer: wandb #For now, only wandb